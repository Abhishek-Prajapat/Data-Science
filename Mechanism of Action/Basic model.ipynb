{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/train_features.csv')\n",
    "train_targets = pd.read_csv('../input/train_targets_scored.csv')\n",
    "test_features = pd.read_csv('../input/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose    g-0     g-1     g-2     g-3  \\\n",
       "0  id_000644bb2  trt_cp       24      D1  1.062  0.5577 -0.2479 -0.6208   \n",
       "\n",
       "      g-4    g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n",
       "0 -0.1944 -1.012  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n",
       "\n",
       "     c-97    c-98    c-99  \n",
       "0  0.2139  0.3801  0.4176  \n",
       "\n",
       "[1 rows x 876 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fold(data):\n",
    "    \n",
    "    data['fold'] = -1\n",
    "    \n",
    "    data = data.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    targets = data.drop('sig_id', axis=1).values\n",
    "    \n",
    "    splitter = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
    "    \n",
    "    for fold, (train, valid) in enumerate(splitter.split(X=data, y=targets)):\n",
    "        \n",
    "        data.loc[valid, 'fold'] = fold\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naruto/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass shuffle=False, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "/home/naruto/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "train_targets = create_fold(train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.to_csv('../output/fold_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    cp_time = pd.get_dummies(data['cp_time'])\n",
    "    cp_type = pd.get_dummies(data['cp_type'])\n",
    "    cp_dose = pd.get_dummies(data['cp_dose'])\n",
    "    \n",
    "    \n",
    "    data = data.join(cp_time)\n",
    "    data = data.join(cp_type)\n",
    "    data = data.join(cp_dose)\n",
    "    \n",
    "    data.drop(columns = ['cp_time', 'cp_dose', 'cp_type'], inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = preprocess(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23814, 1087), (23814, 880), (23814, 208))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_features.merge(train_targets, on='sig_id', how='outer')\n",
    "train_df.shape, train_features.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_columns):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Input(num_columns),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n",
    "                  loss='binary_crossentropy', \n",
    "                  )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for _target in train_targets.columns[1:-1]:\n",
    "        \n",
    "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
    "        \n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    \n",
    "    model = create_model(879)\n",
    "    \n",
    "    cheakpoint_path = f'Fold_{fold}.hdf5'\n",
    "\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "    cb_checkpt = ModelCheckpoint(cheakpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "    \n",
    "    \n",
    "    #==============================================\n",
    "    \n",
    "    cols = train_df.columns.values\n",
    "    cols = cols[1:880]\n",
    "    \n",
    "    tar = train_df.columns.values\n",
    "    tar = tar[880:-1]\n",
    "    \n",
    "    x_train = train_df[train_df['fold'] != fold]\n",
    "    x_train = x_train.loc[:, cols]\n",
    "    \n",
    "    y_train = train_df[train_df['fold'] != fold]\n",
    "    y_train = y_train.loc[:, tar]\n",
    " \n",
    "    x_valid = train_df[train_df['fold'] == fold]\n",
    "    x_valid = x_valid.loc[:, cols]\n",
    "    \n",
    "    y_valid = train_df[train_df['fold'] == fold]\n",
    "    y_valid = y_valid.loc[:, tar]    \n",
    "    \n",
    "    #==============================================\n",
    "    \n",
    "    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n",
    "             callbacks = [reduce_lr_loss, cb_checkpt], verbose=2)\n",
    "    \n",
    "    #==============================================\n",
    "    \n",
    "    model.load_weights(cheakpoint_path)\n",
    "    \n",
    "    y_pred = model.predict(x_valid)\n",
    "    \n",
    "    y_pred = pd.DataFrame(y_pred, columns = y_valid.columns)\n",
    "    \n",
    "    print('Metric: ', metric(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/35\n",
      "WARNING:tensorflow:From /home/naruto/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "168/168 - 2s - loss: 0.5639 - val_loss: 0.2124 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "168/168 - 2s - loss: 0.1137 - val_loss: 0.0449 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "168/168 - 2s - loss: 0.0377 - val_loss: 0.0275 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "168/168 - 2s - loss: 0.0261 - val_loss: 0.0228 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "168/168 - 2s - loss: 0.0225 - val_loss: 0.0203 - lr: 0.0010\n",
      "Epoch 6/35\n",
      "168/168 - 2s - loss: 0.0206 - val_loss: 0.0189 - lr: 0.0010\n",
      "Epoch 7/35\n",
      "168/168 - 2s - loss: 0.0196 - val_loss: 0.0183 - lr: 0.0010\n",
      "Epoch 8/35\n",
      "168/168 - 2s - loss: 0.0188 - val_loss: 0.0178 - lr: 0.0010\n",
      "Epoch 9/35\n",
      "168/168 - 2s - loss: 0.0184 - val_loss: 0.0173 - lr: 0.0010\n",
      "Epoch 10/35\n",
      "168/168 - 2s - loss: 0.0179 - val_loss: 0.0170 - lr: 0.0010\n",
      "Epoch 11/35\n",
      "168/168 - 2s - loss: 0.0175 - val_loss: 0.0166 - lr: 0.0010\n",
      "Epoch 12/35\n",
      "168/168 - 2s - loss: 0.0171 - val_loss: 0.0164 - lr: 0.0010\n",
      "Epoch 13/35\n",
      "168/168 - 2s - loss: 0.0169 - val_loss: 0.0162 - lr: 0.0010\n",
      "Epoch 14/35\n",
      "168/168 - 2s - loss: 0.0168 - val_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 15/35\n",
      "168/168 - 2s - loss: 0.0164 - val_loss: 0.0158 - lr: 0.0010\n",
      "Epoch 16/35\n",
      "168/168 - 2s - loss: 0.0162 - val_loss: 0.0157 - lr: 0.0010\n",
      "Epoch 17/35\n",
      "168/168 - 2s - loss: 0.0159 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 18/35\n",
      "168/168 - 2s - loss: 0.0158 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 19/35\n",
      "168/168 - 2s - loss: 0.0155 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 20/35\n",
      "168/168 - 2s - loss: 0.0155 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 21/35\n",
      "168/168 - 2s - loss: 0.0152 - val_loss: 0.0152 - lr: 0.0010\n",
      "Epoch 22/35\n",
      "168/168 - 2s - loss: 0.0149 - val_loss: 0.0151 - lr: 0.0010\n",
      "Epoch 23/35\n",
      "168/168 - 2s - loss: 0.0147 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 24/35\n",
      "168/168 - 2s - loss: 0.0145 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 25/35\n",
      "168/168 - 2s - loss: 0.0142 - val_loss: 0.0149 - lr: 0.0010\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "168/168 - 2s - loss: 0.0140 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 27/35\n",
      "168/168 - 3s - loss: 0.0135 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 28/35\n",
      "168/168 - 2s - loss: 0.0134 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 29/35\n",
      "168/168 - 2s - loss: 0.0133 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "168/168 - 2s - loss: 0.0133 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 31/35\n",
      "168/168 - 3s - loss: 0.0132 - val_loss: 0.0148 - lr: 1.0000e-05\n",
      "Epoch 32/35\n",
      "168/168 - 3s - loss: 0.0132 - val_loss: 0.0148 - lr: 1.0000e-05\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "168/168 - 2s - loss: 0.0132 - val_loss: 0.0148 - lr: 1.0000e-05\n",
      "Epoch 34/35\n",
      "168/168 - 2s - loss: 0.0132 - val_loss: 0.0148 - lr: 1.0000e-06\n",
      "Epoch 35/35\n",
      "168/168 - 3s - loss: 0.0132 - val_loss: 0.0148 - lr: 1.0000e-06\n",
      "Metric:  0.01475484240672386\n"
     ]
    }
   ],
   "source": [
    "run(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
