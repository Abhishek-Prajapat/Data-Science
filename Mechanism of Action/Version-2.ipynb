{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/train_features.csv')\n",
    "train_targets = pd.read_csv('../input/train_targets_scored.csv')\n",
    "train_non_targets = pd.read_csv('../input/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../input/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.062</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose    g-0     g-1     g-2     g-3  \\\n",
       "0  id_000644bb2  trt_cp       24      D1  1.062  0.5577 -0.2479 -0.6208   \n",
       "\n",
       "      g-4    g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95    c-96  \\\n",
       "0 -0.1944 -1.012  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584 -0.3981   \n",
       "\n",
       "     c-97    c-98    c-99  \n",
       "0  0.2139  0.3801  0.4176  \n",
       "\n",
       "[1 rows x 876 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>abc_transporter_expression_enhancer</th>\n",
       "      <th>abl_inhibitor</th>\n",
       "      <th>ace_inhibitor</th>\n",
       "      <th>acetylcholine_release_enhancer</th>\n",
       "      <th>adenosine_deaminase_inhibitor</th>\n",
       "      <th>adenosine_kinase_inhibitor</th>\n",
       "      <th>adenylyl_cyclase_inhibitor</th>\n",
       "      <th>age_inhibitor</th>\n",
       "      <th>alcohol_dehydrogenase_inhibitor</th>\n",
       "      <th>...</th>\n",
       "      <th>ve-cadherin_antagonist</th>\n",
       "      <th>vesicular_monoamine_transporter_inhibitor</th>\n",
       "      <th>vitamin_k_antagonist</th>\n",
       "      <th>voltage-gated_calcium_channel_ligand</th>\n",
       "      <th>voltage-gated_potassium_channel_activator</th>\n",
       "      <th>voltage-gated_sodium_channel_blocker</th>\n",
       "      <th>wdr5_mll_interaction_inhibitor</th>\n",
       "      <th>wnt_agonist</th>\n",
       "      <th>xanthine_oxidase_inhibitor</th>\n",
       "      <th>xiap_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  abc_transporter_expression_enhancer  abl_inhibitor  \\\n",
       "0  id_000644bb2                                    0              0   \n",
       "\n",
       "   ace_inhibitor  acetylcholine_release_enhancer  \\\n",
       "0              0                               0   \n",
       "\n",
       "   adenosine_deaminase_inhibitor  adenosine_kinase_inhibitor  \\\n",
       "0                              0                           0   \n",
       "\n",
       "   adenylyl_cyclase_inhibitor  age_inhibitor  alcohol_dehydrogenase_inhibitor  \\\n",
       "0                           0              0                                0   \n",
       "\n",
       "   ...  ve-cadherin_antagonist  vesicular_monoamine_transporter_inhibitor  \\\n",
       "0  ...                       0                                          0   \n",
       "\n",
       "   vitamin_k_antagonist  voltage-gated_calcium_channel_ligand  \\\n",
       "0                     0                                     0   \n",
       "\n",
       "   voltage-gated_potassium_channel_activator  \\\n",
       "0                                          0   \n",
       "\n",
       "   voltage-gated_sodium_channel_blocker  wdr5_mll_interaction_inhibitor  \\\n",
       "0                                     0                               0   \n",
       "\n",
       "   wnt_agonist  xanthine_oxidase_inhibitor  xiap_inhibitor  \n",
       "0            0                           0               0  \n",
       "\n",
       "[1 rows x 403 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_non_targets.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Non-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>abc_transporter_expression_enhancer</th>\n",
       "      <th>abl_inhibitor</th>\n",
       "      <th>ace_inhibitor</th>\n",
       "      <th>acetylcholine_release_enhancer</th>\n",
       "      <th>adenosine_deaminase_inhibitor</th>\n",
       "      <th>adenosine_kinase_inhibitor</th>\n",
       "      <th>adenylyl_cyclase_inhibitor</th>\n",
       "      <th>age_inhibitor</th>\n",
       "      <th>alcohol_dehydrogenase_inhibitor</th>\n",
       "      <th>...</th>\n",
       "      <th>ve-cadherin_antagonist</th>\n",
       "      <th>vesicular_monoamine_transporter_inhibitor</th>\n",
       "      <th>vitamin_k_antagonist</th>\n",
       "      <th>voltage-gated_calcium_channel_ligand</th>\n",
       "      <th>voltage-gated_potassium_channel_activator</th>\n",
       "      <th>voltage-gated_sodium_channel_blocker</th>\n",
       "      <th>wdr5_mll_interaction_inhibitor</th>\n",
       "      <th>wnt_agonist</th>\n",
       "      <th>xanthine_oxidase_inhibitor</th>\n",
       "      <th>xiap_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  abc_transporter_expression_enhancer  abl_inhibitor  \\\n",
       "0  id_000644bb2                                    0              0   \n",
       "\n",
       "   ace_inhibitor  acetylcholine_release_enhancer  \\\n",
       "0              0                               0   \n",
       "\n",
       "   adenosine_deaminase_inhibitor  adenosine_kinase_inhibitor  \\\n",
       "0                              0                           0   \n",
       "\n",
       "   adenylyl_cyclase_inhibitor  age_inhibitor  alcohol_dehydrogenase_inhibitor  \\\n",
       "0                           0              0                                0   \n",
       "\n",
       "   ...  ve-cadherin_antagonist  vesicular_monoamine_transporter_inhibitor  \\\n",
       "0  ...                       0                                          0   \n",
       "\n",
       "   vitamin_k_antagonist  voltage-gated_calcium_channel_ligand  \\\n",
       "0                     0                                     0   \n",
       "\n",
       "   voltage-gated_potassium_channel_activator  \\\n",
       "0                                          0   \n",
       "\n",
       "   voltage-gated_sodium_channel_blocker  wdr5_mll_interaction_inhibitor  \\\n",
       "0                                     0                               0   \n",
       "\n",
       "   wnt_agonist  xanthine_oxidase_inhibitor  xiap_inhibitor  \n",
       "0            0                           0               0  \n",
       "\n",
       "[1 rows x 403 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_non_targets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = np.sum(train_non_targets.iloc[:, 1:], axis = 0)\n",
    "\n",
    "sns.distplot(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_fold(data):\n",
    "    \n",
    "    data['fold'] = -1\n",
    "    \n",
    "    data = data.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    targets = data.drop('sig_id', axis=1).values\n",
    "    \n",
    "    splitter = MultilabelStratifiedKFold(n_splits=7, random_state=0)\n",
    "    \n",
    "    for fold, (train, valid) in enumerate(splitter.split(X=data, y=targets)):\n",
    "        \n",
    "        data.loc[valid, 'fold'] = fold\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined = train_targets.merge(train_non_targets, on='sig_id', how='outer')\n",
    "combined = create_fold(combined)\n",
    "train_targets['fold'] = combined['fold']\n",
    "train_non_targets['fold'] = combined['fold']\n",
    "\n",
    "del(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_targets.to_csv('../output/fold_data_targets.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_non_targets.to_csv('../output/fold_data_non_targets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    cp_time = pd.get_dummies(data['cp_time'])\n",
    "    cp_type = pd.get_dummies(data['cp_type'])\n",
    "    cp_dose = pd.get_dummies(data['cp_dose'])\n",
    "    \n",
    "    \n",
    "    data = data.join(cp_time)\n",
    "    data = data.join(cp_type)\n",
    "    data = data.join(cp_dose)\n",
    "    \n",
    "    data.drop(columns = ['cp_time', 'cp_dose', 'cp_type'], inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_features = preprocess(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def uni_non_targets():\n",
    "    train_df = train_features.merge(train_non_targets, on='sig_id', how='outer')   \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(num_inputs, num_outputs):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Input(num_inputs),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_outputs, activation=\"sigmoid\"))\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n",
    "                  loss='binary_crossentropy', \n",
    "                  )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for _target in train_targets.columns[1:-1]:\n",
    "        \n",
    "        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n",
    "        \n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we will make a simple model and use it to predict the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = train_features.merge(train_targets, on='sig_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_model(fold):\n",
    "    \n",
    "    train_df = train_features.merge(train_targets, on='sig_id', how='outer')\n",
    "    \n",
    "    # defining the parameters\n",
    "    cols = train_df.columns\n",
    "    \n",
    "    ID = cols[0]\n",
    "    fold_col = cols[-1]\n",
    "    features = cols[1:880]\n",
    "    # we are skipping the mid (947th column) as it is the fold column of the previous part\n",
    "    targets = cols[880:-1]\n",
    "    \n",
    "    # loading the data\n",
    "    train = train_df[train_df['fold'] != fold]\n",
    "    valid = train_df[train_df['fold'] == fold]\n",
    "    \n",
    "    x_train = train.loc[:, features]\n",
    "    x_valid = valid.loc[:, features]\n",
    "    \n",
    "    y_train = train.loc[:, targets]\n",
    "    y_valid = valid.loc[:, targets]\n",
    "    \n",
    "    # creating the model\n",
    "    model = create_model(x_train.shape[1], y_train.shape[1])\n",
    "    \n",
    "    # Defining model callbacks\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "    \n",
    "    checkpoint_path = f'Fold_{fold}_basic.hdf5'\n",
    "    \n",
    "    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n",
    "             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n",
    "    \n",
    "    # Loading the best model\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Making the predictions\n",
    "    y_valid_pred = model.predict(x_valid)\n",
    "    \n",
    "    # converting the predictions to dataframe\n",
    "    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns)\n",
    "    \n",
    "    # Evaluating the final results\n",
    "    print('\\n\\n\\n')\n",
    "    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/35\n",
      "WARNING:tensorflow:From /home/naruto/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.5839 - val_loss: 0.2814 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.1310 - val_loss: 0.0545 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0418 - val_loss: 0.0285 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0277 - val_loss: 0.0230 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0231 - val_loss: 0.0206 - lr: 0.0010\n",
      "Epoch 6/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0210 - val_loss: 0.0193 - lr: 0.0010\n",
      "Epoch 7/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0198 - val_loss: 0.0186 - lr: 0.0010\n",
      "Epoch 8/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0191 - val_loss: 0.0180 - lr: 0.0010\n",
      "Epoch 9/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0185 - val_loss: 0.0175 - lr: 0.0010\n",
      "Epoch 10/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0180 - val_loss: 0.0171 - lr: 0.0010\n",
      "Epoch 11/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0178 - val_loss: 0.0169 - lr: 0.0010\n",
      "Epoch 12/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0176 - val_loss: 0.0170 - lr: 0.0010\n",
      "Epoch 13/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0172 - val_loss: 0.0164 - lr: 0.0010\n",
      "Epoch 14/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0169 - val_loss: 0.0163 - lr: 0.0010\n",
      "Epoch 15/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0165 - val_loss: 0.0161 - lr: 0.0010\n",
      "Epoch 16/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0163 - val_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 17/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0160 - val_loss: 0.0157 - lr: 0.0010\n",
      "Epoch 18/35\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0159 - val_loss: 0.0158 - lr: 0.0010\n",
      "Epoch 19/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0157 - val_loss: 0.0157 - lr: 0.0010\n",
      "Epoch 20/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0155 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 21/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0152 - val_loss: 0.0154 - lr: 0.0010\n",
      "Epoch 22/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0152 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 23/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0148 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 24/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0146 - val_loss: 0.0152 - lr: 0.0010\n",
      "Epoch 25/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0144 - val_loss: 0.0151 - lr: 0.0010\n",
      "Epoch 26/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0145 - val_loss: 0.0151 - lr: 0.0010\n",
      "Epoch 27/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0141 - val_loss: 0.0151 - lr: 0.0010\n",
      "Epoch 28/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0139 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 29/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0136 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 30/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0134 - val_loss: 0.0150 - lr: 0.0010\n",
      "Epoch 31/35\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0131\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0131 - val_loss: 0.0149 - lr: 0.0010\n",
      "Epoch 32/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0125 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 33/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0125 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 34/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0123 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "Epoch 35/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0122 - val_loss: 0.0148 - lr: 1.0000e-04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OOF Metric:  0.014790616317578616\n"
     ]
    }
   ],
   "source": [
    "run_model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this we will make a model to predict the train_non_scored columns and then we will merge that with the data we have as features and then we will use that whole data as our feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_part1(fold):\n",
    "    \n",
    "    # loading the targets_non_scored concatinated data\n",
    "    train_df = uni_non_targets()\n",
    "    \n",
    "    # defining the parameters\n",
    "    cols = train_df.columns\n",
    "    \n",
    "    ID = cols[0]\n",
    "    fold_col = cols[-1]\n",
    "    features = cols[1:880]\n",
    "    targets = cols[880:-1]\n",
    "    \n",
    "    # loading the data\n",
    "    train = train_df[train_df['fold'] != fold]\n",
    "    valid = train_df[train_df['fold'] == fold]\n",
    "    \n",
    "    x_train = train.loc[:, features]\n",
    "    x_valid = valid.loc[:, features]\n",
    "    \n",
    "    y_train = train.loc[:, targets]\n",
    "    y_valid = valid.loc[:, targets]\n",
    "    \n",
    "    # creating the model\n",
    "    model = create_model(x_train.shape[1], y_train.shape[1])\n",
    "    \n",
    "    # Defining model callbacks\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "    \n",
    "    checkpoint_path = f'Fold_{fold}_part1.hdf5'\n",
    "    \n",
    "    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n",
    "             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n",
    "    \n",
    "    # Loading the best weights model\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Making the predictions\n",
    "    y_valid_pred = model.predict(x_valid)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    \n",
    "    # converting the y_preds\n",
    "    y_valid_pred = pd.DataFrame(y_valid_pred, columns = y_valid.columns)\n",
    "    y_train_pred = pd.DataFrame(y_train_pred, columns = y_valid.columns)\n",
    "    \n",
    "    # replacing the train_df with the predicted data\n",
    "    train_df.loc[:, targets][train_df['fold'] != fold] = y_train_pred\n",
    "    train_df.loc[:, targets][train_df['fold'] == fold] = y_valid_pred\n",
    "    \n",
    "    # drop this fold\n",
    "    train_df.drop(columns='fold', inplace=True)\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_part2(fold):\n",
    "    \n",
    "    # Prepairing final data\n",
    "    features = run_part1(fold)\n",
    "    targets = train_targets\n",
    "    \n",
    "    # Merging both\n",
    "    train_df = features.merge(targets, on='sig_id', how='outer')\n",
    "    \n",
    "    # defining the parameters\n",
    "    cols = train_df.columns\n",
    "    \n",
    "    ID = cols[0]\n",
    "    fold_col = cols[-1]\n",
    "    features = cols[1:947]\n",
    "    # we are skipping the mid (947th column) as it is the fold column of the previous part\n",
    "    targets = cols[948:-1]\n",
    "    \n",
    "    # loading the data\n",
    "    train = train_df[train_df['fold'] != fold]\n",
    "    valid = train_df[train_df['fold'] == fold]\n",
    "    \n",
    "    x_train = train.loc[:, features]\n",
    "    x_valid = valid.loc[:, features]\n",
    "    \n",
    "    y_train = train.loc[:, targets]\n",
    "    y_valid = valid.loc[:, targets]\n",
    "    \n",
    "    # creating the model\n",
    "    model = create_model(x_train.shape[1], y_train.shape[1])\n",
    "    \n",
    "    # Defining model callbacks\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "    \n",
    "    checkpoint_path = f'Fold_{fold}_part1.hdf5'\n",
    "    \n",
    "    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "    \n",
    "    # Some blanck lines\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n",
    "             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n",
    "    \n",
    "    # Loading the best model\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Making the predictions\n",
    "    y_valid_pred = model.predict(x_valid)\n",
    "    \n",
    "    # converting the predictions to dataframe\n",
    "    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns)\n",
    "    \n",
    "    # Evaluating the final results\n",
    "    print('\\n\\n\\n')\n",
    "    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    \n",
    "    run_part2(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.5768 - val_loss: 0.2549 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.1123 - val_loss: 0.0402 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0279 - val_loss: 0.0174 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0140 - val_loss: 0.0109 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0095 - val_loss: 0.0074 - lr: 0.0010\n",
      "Epoch 6/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0075 - val_loss: 0.0064 - lr: 0.0010\n",
      "Epoch 7/35\n",
      "160/160 [==============================] - 5s 32ms/step - loss: 0.0063 - val_loss: 0.0055 - lr: 0.0010\n",
      "Epoch 8/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0057 - val_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 9/35\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.0052 - val_loss: 0.0049 - lr: 0.0010\n",
      "Epoch 10/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0050 - val_loss: 0.0048 - lr: 0.0010\n",
      "Epoch 11/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0048 - val_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 12/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0046 - val_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 13/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0045 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 14/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0044 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 15/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0044 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 16/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0043 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 17/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0043 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 18/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0043 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 19/35\n",
      "156/160 [============================>.] - ETA: 0s - loss: 0.0044\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0044 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 20/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 21/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0042 - val_loss: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 22/35\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-04\n",
      "Epoch 23/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-05\n",
      "Epoch 24/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-05\n",
      "Epoch 25/35\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.0041 - val_loss: 0.0042 - lr: 1.0000e-05\n",
      "Epoch 26/35\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-06\n",
      "Epoch 27/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-06\n",
      "Epoch 28/35\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-06\n",
      "Epoch 29/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-07\n",
      "Epoch 30/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-07\n",
      "Epoch 31/35\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-07\n",
      "Epoch 32/35\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.0041 - val_loss: 0.0042 - lr: 1.0000e-08\n",
      "Epoch 33/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-08\n",
      "Epoch 34/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0042\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-08\n",
      "Epoch 35/35\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0042 - val_loss: 0.0042 - lr: 1.0000e-09\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.5744 - val_loss: 0.2644 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.1127 - val_loss: 0.0445 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0313 - val_loss: 0.0213 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "160/160 [==============================] - 2s 16ms/step - loss: 0.0184 - val_loss: 0.0149 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0149 - val_loss: 0.0127 - lr: 0.0010\n",
      "Epoch 6/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0127 - val_loss: 0.0115 - lr: 0.0010\n",
      "Epoch 7/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0117 - val_loss: 0.0107 - lr: 0.0010\n",
      "Epoch 8/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0110 - val_loss: 0.0105 - lr: 0.0010\n",
      "Epoch 9/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0105 - val_loss: 0.0099 - lr: 0.0010\n",
      "Epoch 10/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0103 - val_loss: 0.0099 - lr: 0.0010\n",
      "Epoch 11/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0100 - val_loss: 0.0096 - lr: 0.0010\n",
      "Epoch 12/35\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0098 - val_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 13/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0098 - val_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 14/35\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.0095 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 15/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0094 - val_loss: 0.0091 - lr: 0.0010\n",
      "Epoch 16/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0093 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 17/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0092 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 18/35\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.0090 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 19/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0089 - val_loss: 0.0087 - lr: 0.0010\n",
      "Epoch 20/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0088 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 21/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0087 - val_loss: 0.0086 - lr: 0.0010\n",
      "Epoch 22/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0086 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 23/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0085 - val_loss: 0.0084 - lr: 0.0010\n",
      "Epoch 24/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0084 - val_loss: 0.0084 - lr: 0.0010\n",
      "Epoch 25/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0083 - val_loss: 0.0083 - lr: 0.0010\n",
      "Epoch 26/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0082 - val_loss: 0.0083 - lr: 0.0010\n",
      "Epoch 27/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0082 - val_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 28/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0081 - val_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 29/35\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.0080 - val_loss: 0.0081 - lr: 0.0010\n",
      "Epoch 30/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0082\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.0082 - val_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 31/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0078 - val_loss: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 32/35\n",
      "160/160 [==============================] - 3s 16ms/step - loss: 0.0078 - val_loss: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 33/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0078 - val_loss: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 34/35\n",
      "160/160 [==============================] - 3s 17ms/step - loss: 0.0077 - val_loss: 0.0081 - lr: 1.0000e-04\n",
      "Epoch 35/35\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0077 - val_loss: 0.0081 - lr: 1.0000e-04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OOF Metric:  0.014998730334162283\n"
     ]
    }
   ],
   "source": [
    "run(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although The OOF Metric didn't changed for the better but if you look at the validation_loss you will see that it is lower in Method2 in compare to Method1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based upon the term \"Gene Sequence\". What if the Sequence could give us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Seq_model(num_inputs, num_outputs):\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1024), input_shape=(1, num_inputs)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation=\"relu\")),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_outputs, activation=\"sigmoid\"))\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10),\n",
    "                  loss='binary_crossentropy', \n",
    "                  )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_seq_model(fold):\n",
    "    \n",
    "    train_df = train_features.merge(train_targets, on='sig_id', how='outer')\n",
    "    \n",
    "    # defining the parameters\n",
    "    cols = train_df.columns\n",
    "    \n",
    "    ID = cols[0]\n",
    "    fold_col = cols[-1]\n",
    "    features = cols[1:880]\n",
    "    # we are skipping the mid (947th column) as it is the fold column of the previous part\n",
    "    targets = cols[880:-1]\n",
    "    \n",
    "    # loading the data\n",
    "    train = train_df[train_df['fold'] != fold]\n",
    "    valid = train_df[train_df['fold'] == fold]\n",
    "    \n",
    "    x_train = train.loc[:, features]\n",
    "    x_valid = valid.loc[:, features]\n",
    "    \n",
    "    y_train = train.loc[:, targets]\n",
    "    y_valid = valid.loc[:, targets]\n",
    "    \n",
    "    \n",
    "    # reshaping the data for LSTM\n",
    "    x_train = np.array(x_train).reshape(-1, 1, 879)\n",
    "    x_valid = np.array(x_valid).reshape(-1, 1, 879)\n",
    "    \n",
    "    # creating the model\n",
    "    model = Seq_model(x_train.shape[2], y_train.shape[1])\n",
    "    \n",
    "    # Defining model callbacks\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "    \n",
    "    checkpoint_path = f'Fold_{fold}_Seq.hdf5'\n",
    "    \n",
    "    cb_checkpt = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                save_weights_only=True, mode='min')\n",
    "    \n",
    "    # Fitting the model\n",
    "    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=35, batch_size=128,\n",
    "             callbacks = [reduce_lr_loss, cb_checkpt], verbose=1)\n",
    "    \n",
    "    # Loading the best model\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    # Making the predictions\n",
    "    y_valid_pred = model.predict(x_valid)\n",
    "    \n",
    "    # converting the predictions to dataframe\n",
    "    y_valid_pred = pd.DataFrame(y_valid_pred, columns=y_valid.columns)\n",
    "    \n",
    "    # Evaluating the final results\n",
    "    print('\\n\\n\\n')\n",
    "    print('OOF Metric: ', metric(y_valid, y_valid_pred))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.5830 - val_loss: 0.2603 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "160/160 [==============================] - 5s 34ms/step - loss: 0.1283 - val_loss: 0.0463 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "160/160 [==============================] - 6s 34ms/step - loss: 0.0406 - val_loss: 0.0274 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "160/160 [==============================] - 6s 34ms/step - loss: 0.0270 - val_loss: 0.0220 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0230 - val_loss: 0.0201 - lr: 0.0010\n",
      "Epoch 6/35\n",
      "160/160 [==============================] - 6s 35ms/step - loss: 0.0208 - val_loss: 0.0187 - lr: 0.0010\n",
      "Epoch 7/35\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0196 - val_loss: 0.0180 - lr: 0.0010\n",
      "Epoch 8/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0186 - val_loss: 0.0173 - lr: 0.0010\n",
      "Epoch 9/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0180 - val_loss: 0.0171 - lr: 0.0010\n",
      "Epoch 10/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0176 - val_loss: 0.0167 - lr: 0.0010\n",
      "Epoch 11/35\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0170 - val_loss: 0.0164 - lr: 0.0010\n",
      "Epoch 12/35\n",
      "160/160 [==============================] - 6s 37ms/step - loss: 0.0166 - val_loss: 0.0162 - lr: 0.0010\n",
      "Epoch 13/35\n",
      "160/160 [==============================] - 6s 40ms/step - loss: 0.0162 - val_loss: 0.0160 - lr: 0.0010\n",
      "Epoch 14/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0158 - val_loss: 0.0158 - lr: 0.0010\n",
      "Epoch 15/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0154 - val_loss: 0.0156 - lr: 0.0010\n",
      "Epoch 16/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0149 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 17/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0146 - val_loss: 0.0154 - lr: 0.0010\n",
      "Epoch 18/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0141 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 19/35\n",
      "160/160 [==============================] - 7s 43ms/step - loss: 0.0135 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 20/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0131 - val_loss: 0.0152 - lr: 0.0010\n",
      "Epoch 21/35\n",
      "160/160 [==============================] - 6s 36ms/step - loss: 0.0125 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 22/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0118 - val_loss: 0.0154 - lr: 0.0010\n",
      "Epoch 23/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0111\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0111 - val_loss: 0.0153 - lr: 0.0010\n",
      "Epoch 24/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0099 - val_loss: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 25/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0097 - val_loss: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 26/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0094 - val_loss: 0.0152 - lr: 1.0000e-04\n",
      "Epoch 27/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0093 - val_loss: 0.0152 - lr: 1.0000e-05\n",
      "Epoch 28/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-05\n",
      "Epoch 29/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-05\n",
      "Epoch 30/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-06\n",
      "Epoch 31/35\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-06\n",
      "Epoch 32/35\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.0092\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-06\n",
      "Epoch 33/35\n",
      "160/160 [==============================] - 6s 39ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-07\n",
      "Epoch 34/35\n",
      "160/160 [==============================] - 7s 47ms/step - loss: 0.0092 - val_loss: 0.0152 - lr: 1.0000e-07\n",
      "Epoch 35/35\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0091\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "160/160 [==============================] - 6s 38ms/step - loss: 0.0091 - val_loss: 0.0152 - lr: 1.0000e-07\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OOF Metric:  0.015162701940936977\n"
     ]
    }
   ],
   "source": [
    "run_seq_model(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                          0.5                     0.5   \n",
       "1  id_001897cda                          0.5                     0.5   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0             0.5                             0.5   \n",
       "1             0.5                             0.5   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                                0.5                             0.5   \n",
       "1                                0.5                             0.5   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                         0.5                            0.5   \n",
       "1                         0.5                            0.5   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                         0.5  ...                                    0.5   \n",
       "1                         0.5  ...                                    0.5   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0           0.5              0.5                0.5   \n",
       "1           0.5              0.5                0.5   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                        0.5                                    0.5   \n",
       "1                        0.5                                    0.5   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0              0.5        0.5                         0.5            0.5  \n",
       "1              0.5        0.5                         0.5            0.5  \n",
       "\n",
       "[2 rows x 207 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.2067</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-0.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-1.031</td>\n",
       "      <td>-1.3670</td>\n",
       "      <td>-0.3690</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.381</td>\n",
       "      <td>-0.730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n",
       "0  id_0004d9e33  trt_cp       24      D1 -0.5458  0.1306 -0.5135  0.4408   \n",
       "1  id_001897cda  trt_cp       72      D1 -0.1829  0.2320  1.2080 -0.4522   \n",
       "\n",
       "      g-4     g-5  ...    c-90    c-91   c-92    c-93    c-94    c-95    c-96  \\\n",
       "0  1.5500 -0.1644  ...  0.0981  0.7978 -0.143 -0.2067 -0.2303 -0.1193  0.0210   \n",
       "1 -0.3652 -0.3319  ... -0.1190 -0.1852 -1.031 -1.3670 -0.3690 -0.5382  0.0359   \n",
       "\n",
       "     c-97   c-98   c-99  \n",
       "0 -0.0502  0.151 -0.775  \n",
       "1 -0.4764 -1.381 -0.730  \n",
       "\n",
       "[2 rows x 876 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = preprocess(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(879, 206)\n",
    "\n",
    "model.load_weights('Fold_0_basic.hdf5')\n",
    "\n",
    "pred = model.predict(test_features.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred, columns = train_targets.columns.values[1:-1])\n",
    "\n",
    "sub_file1 = sample_submission.copy()\n",
    "sub_file1.iloc[:, 1:] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file1.to_csv('sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
